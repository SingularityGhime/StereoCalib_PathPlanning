# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15k6J4tdD8rtfsg5GRSem4BDCx4Yt6c50
"""

# imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import cv2
from PIL import Image
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from skimage.transform import resize


from tensorflow.keras.utils import to_categorical
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
from keras.models import Sequential

#drive for colab
from google.colab import drive
drive.mount('/gdrive')

# loading dataset
database = [] 
labels = []     #labels
classes = 43    #classes for signs
path_train = '/gdrive/archive/Train'      #training path

# goes to the directory, append  image databse and labels
for i in os.listdir(path_train):
    dir = path_train + '/' + i
    for j in os.listdir(dir):
        img_path = dir+'/'+j
        img = cv2.imread(img_path,-1)
        img = cv2.resize(img, (30,30), interpolation = cv2.INTER_NEAREST)
        database.append(img)
        labels.append(i)

# converting to array        
database = np.array(database)
labels = np.array(labels)
print(database.shape, labels.shape)

trainx, testx, trainy, testy = train_test_split(database, labels, test_size= 0.25, random_state=21) #spilitting  database in 75% train, 25% test
print((trainx.shape, trainy.shape), (testx.shape, testy.shape))  
# encodding for labels
trainy = to_categorical(trainy, 43)
testy = to_categorical(testy, 43)

# defining model structure
model = Sequential()
model.add(Conv2D(32, (5,5), 'relu', input = (30,30,3))) #32 5*5 filter with lineraization activation
model.add(Conv2D(32, (5,5), 'relu'))  #32 5*5 filter with lineraization activation
model.add(MaxPool2D(pool_size = (2,2))) # max pooling with 2*2 and 1 slide
model.add(Dropout(rate = 0.25)) # 0.25 dropout layer

model.add(Conv2D(64, (3,3), 'relu'))    #64 3*3 filter
model.add(Conv2D(64, (3,3), 'relu'))    # 64 3 * 3 filters
model.add(MaxPool2D((2,2))) # max pooling kernel=2, stride = 1
model.add(Dropout(0.25))  #dropout layer 0.25

model.add(Flatten())  # flatten layers
model.add(Dense(256, 'relu'))   #dense fc layer with linerazation activation
model.add(Dropout(0.5))   #0.5 dropout
model.add(Dense(43, 'softmax')) # softmax activation

# compile above models
model.compile(loss = 'categorical_crossentropy', optimizer ='adam', metrics =['accuracy'])

epochs = 18
batch_size =64
history = model.fit(trainx, trainy, 
                   batch_size,
                   epochs=epochs,
                   validation_data=(testx, testy))

testy = pd.read_csv('gdrive/archive/Test.csv')

test_label = testy['ClassId'].values
img_test = testy['Path'].values
test_dir = 'gdrive/archive/'
database = []
for img in img_test:
    img_path = os.path.join(test_dir, img)
#     print(img_path)
    image = cv2.imread(img_path, -1)
    image = cv2.resize(image, (30,30), interpolation = cv2.INTER_NEAREST)
    database.append(np.array(image))
    
testx = np.array(database)

prediction = model.predict_classes(testx)
print("Test accuracy: ", accuracy_score(test_label, prediction) * 100 )